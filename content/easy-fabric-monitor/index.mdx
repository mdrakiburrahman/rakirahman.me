---
title: Alert on thousands of Fabric Pipelines with Monitoring Eventhouse
date: 2025-12-14
published: true
tags: ["Fabric"]
description: Say no to inline Data Factory conditionals, say no to spagghetti
toc: false
seoImage: "og-easy-fabric-monitor.png"
featuredImage: "./featured-image.png"
---

import { Callout } from "../../src/components/atoms.js"
import { ExtLink, InlinePageLink } from "../../src/components/atoms.js"

Say you have a few 1000 Spark Jobs that are wrapped in a few 1000 Pipelines.

In Synapse, you could pipe all execution logs into a Log Analytics workspace, and use [this approach](https://www.c-sharpcorner.com/article/monitor-azure-synapse-analytics-using-log-analytics/) to setup a generic alert that sent a notification on any pipeline failure:

![Alert Configuration in Synapse that fires whenever something fails every 5 minutes](images/alert-config.png)

And get, say, a Teams Alert like this:

![My little angry robot buddy that wakes me up at 3 AM from job failures](images/alert-sample.png)

The official [Fabric Data Factory docs](https://learn.microsoft.com/en-us/fabric/data-factory/create-alerts-for-pipeline-runs) at the time of writing has 2 approaches:

1. Per activity level alert inlined in the pipeline.
2. Using Fabric Activator Job Events, that monitors particular pipelines.

`1` is messy to author and manage.

`2` means you end up having 1000 rules. You can also forget to add a pipeline to a rule and have silent failures.

There are lots of people complaining [on Reddit](https://www.reddit.com/r/MicrosoftFabric/comments/1lk08x9/ideas_data_pipeline_failure_notification/) about how this is difficult. Since the Log Analytics KQL approach worked in Synapse for me, the Fabric Eventhouse KQL approach should work here too.

Let's look at the easiest approach that works for all 1000 pipelines in one shot.

## How to

First, you must [turn on Workspace Monitoring](https://learn.microsoft.com/en-us/fabric/fundamentals/workspace-monitoring-overview) so Fabric Platform level tables are automatically sinked into Eventhouse:

![Turn on workspace monitoring](images/monitoring-on.png)

This `ItemJobEventLogs` from [here](https://learn.microsoft.com/en-us/fabric/fundamentals/item-job-event-logs#considerations) is the important table:

![ItemJobEventLogs contains all pipeline execution events](images/item-job-event-logs.png)

From there, create a rule with this KQL:

```sql
ItemJobEventLogs
| extend SecondsAgo = datetime_diff('second', now(), ingestion_time())
| where JobType == 'Pipeline' and JobStatus == 'Failed' and SecondsAgo <= 540
| order by Timestamp desc
| project Timestamp, SecondsAgo, ItemName, WorkspaceName, JobScheduleTime, JobStartTime, JobEndTime, JobStatus
```


<Callout>

There's a couple gotchas in the KQL above. To avoid getting all historical failures all the time, we set `and SecondsAgo <= 540` (or `9m`). This seemingly random `9m` decision comes with caveats.

</Callout>

What the stateless query assumes is the Reflex itself is always running. It's a single-point-of-failure for your event notifications:

1. The Reflex **must** poll KQL at a faster interval than the lookup (say every `5m`). The repercussion of that is, this alerting engine isn't exactly once, if the Reflex trigger clocks overlap inside the 9 minute window, you get the same alert twice (which is fine, you just need an alert). If the Reflex never fires, you never get an alert that you should have gotten.

   The way to solve this problem is when the Reflex runs, we must store watermark state somewhere and construct the query with it, this isn't trivial since Reflex doesn't have a programmatic interface. We can always use Spark Streaming etc to avoid this problem (since it stores state in checkpoints), but that requires managing yet another SPOF thing.

2. We're using Eventhouse ingestion time as the watermark (and not event time), so even if the ingestion is delayed, we maximize our chances of getting an alert as long as the Reflex runs in the next `9m`.

Note that if Activator Reflex is down - you seem to get this sort of an email - if you get that, you may have had silent failures:

![Activator down](images/activator-down.png)

Create the alert:

![Create an alert](images/alert-me.png)

Fail some demo pipeline:

![A Spark job that blew up](images/stuff-fails.png)

It triggers the Reflex:

![Fires the Alert](images/get-alert.png)

And there's your alert:

![Got the Alert](images/got-alert.png)

It works at scale since the Eventhouse buffers the data, and you can come back and query history easily ðŸ˜Š.

## Caveat

The only criticism I have is I can't embed more emojis into the Fabric Activator Teams Alert ðŸ˜¡.
